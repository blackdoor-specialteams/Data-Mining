import copy
import random
import heapq
from operator import itemgetter

"""
###################################################################################################
###################################################################################################
			General Helper Functions
###################################################################################################
###################################################################################################
"""

def holdout_partition(dataset):
	"""Divides a table into two parts in a 2:1 ratio for length"""
	randomized = dataset [:]
	n = len(dataset)
	for i in range(n):
		j = random.randint(0,n-1)
		randomized[i], randomized[j] = randomized[j],randomized[i]
	n0 = (n*2)/3
	return randomized[0:n0],randomized[n0:]

def k_folds(dataset,k):
	rdm = copy.deepcopy(dataset) 
	n = len(dataset)
	for i in range(n):
		j = random.randint(0,n-1)
		rdm[i], rdm[j] = rdm[j],rdm[i]
	return [rdm[i:i + k] for i in range(0, len(rdm), k)]

def get_mode(xs):
	"""
	Returns the statistical mode of xs
	"""
	counts = dict()
	for x in xs:
		if x in counts:
			counts[x] += 1
		else:
			counts[x] = 1
	ret = xs[0]
	for x in counts.keys():
		if counts[x] > counts[ret]:
			ret = x
	return ret

def get_n_rand_instances(dataset,n):
	size = len(dataset)
	index = sorted([int(random.random()*(size-1)) for _ in range(0, n)])
	result = []
	for i in index:
		result.append(dataset[i])
	return result

def get_mpg_rating(x):
	x = float(x)
	if x >= 45:
		return 10
	elif x >= 37:
		return 9
	elif x >= 31:
		return 8
	elif x >= 27:
		return 7
	elif x >= 24:
		return 6 
	elif x >= 20:
		return 5
	elif x >= 17:
		return 4
	elif x >= 15:
		return 3 
	elif x > 13:
		return 2
	elif x <= 13:
		return 1

def get_NHTSA_rating(y):
	x = float(y)
	if x >= 3500:
		return 5
	elif x > 3000:
		return 4
	elif x > 2500:
		return 3
	elif x > 2000:
		return 2 
	else:
		return 1

def guassian(x,mean,sdev):
	"""returns the guassian distance for a value x"""
	first, second = 0,0
	if sdev > 0:
		first = float(1) / float((math.sqrt(2.0*math.pi) * sdev))
		second = math.e ** (-((x - mean) ** 2) / (2.0 *(sdev ** 2)))
	return first * second

def init_nxn(n):
	"""Creates a 2d matrix that is nxn"""
	l =[]
	for i in range(1,n+1):
		l.append([])
		for j in range(1,n+1):
			l[i - 1].append(0)
	return l

"""
###################################################################################################
			NAIVE BAYES Classifier
###################################################################################################
How to use:
1. Build the "rules" by calling "build_all_class_dicts" on a table, key value and list of attributes
2. pass an instance, a list of attributes and the rules to "nb_classify" and it will return the 
	class label with the highest probablity
"""
def nb_classify(inst,attlist,rules):
	"""comares a instance to a set of probablites generated by Naive Bayes, for an attribute list"""
	prob = []
	for k in rules.keys():
		p = rules[k].get(-1)
		attkeys = rules[k].copy()
		#print attkeys
		attkeys.pop(-1,None) 
		for j in attlist:
		#for every attribute to be compared
			if inst[j] in rules[k][j].keys():
			#if the value is in the key set
				p = float(p) * float(rules[k][j].get(inst[j]))
			else:
				p = float(0.0)
		prob.append((p,k))
	#sort by prob, then return the first elem, which is the key
	prob.sort(key=lambda x: float(x[0]), reverse=True)
	return prob[0][1]

def build_all_class_dicts(table,keycol,attlist):
	""" builds a dict of all classes in the dataset, based on the keycol"""
	keydict,total = get_class_keys_and_counts(table,keycol)
	train_dict = {}
	for k in keydict.keys():
		train_dict.update({k:build_class_dict(table,keycol,k,attlist,keydict.get(k),total)})
	return train_dict

def build_class_dict(table,keycol,clskey,attlist,clscount,total):
	"""builds a dict on a single class for the attlist of the table
	Also appends the probablity of this class as -1"""
	cls = {}
	for x in attlist:
		cls.update({x:build_att_dict_for_class(table,keycol,clskey,x,clscount)})
	#add the probablit of the class
	cls.update({-1: (float(clscount) / float(total))})
	return cls


def build_att_dict_for_class(table,keycol,clskey,col,clscount):
	"""Builds the dict of a single attribute for a class on the column"""
	att_d = {}
	#Build dict with counts for each key
	for row in table:
		if row[keycol] == clskey:
			if row[col] not in att_d.keys():
				att_d.update({row[col]:1})
			else:
				att_d[row[col]] = att_d.get(row[col]) + 1
	#get probablity for each key of the attribute
	clsp = {}
	for k in att_d.keys():
		p = (float(att_d.get(k)) / float(clscount))
		clsp.update({k:p})
	return clsp

"""
###################################################################################################
			Linear Regression Classifier
###################################################################################################
How to Use:
"""

def calculate_best_fit_line(xs, ys):
	if len(xs) != len(ys):
		return None
	_x = float(sum(xs)) / float(len(xs))
	_y = float(sum(ys)) / float(len(ys))
	num = 0
	denom = 0
	for x_point, y_point in zip(xs, ys):
		num += (x_point-_x)*(y_point-_y)
		denom += math.pow(x_point-_x, 2)
	m = num/denom
	return (_x, _y, m)

def lr_classify(_x, _y, m, x = None, y = None):
	if y == None and x == None:
		return None
	if(y == None): # solve for y
		return m * (x - _x) + _y /1.0
	else:# solve for x
		return (y -_y)/m + _x /1.0

"""
###################################################################################################
			K-NN Classifier
###################################################################################################
How to Use:

"""
def knn_classify():
	return None

def get_knn(instance, k, training_set, attribs):
	'''
	Returns the k nearest neighbors to instance based on training_file and attribs
	'''
	ranges = get_ranges(training_set, attribs)
	distances = []
	for inst in training_set:
		distances.append( [get_neighbor_d(instance, inst, ranges, attribs), inst] )
	distances = sorted(distances, key=itemgetter(0))
	ret = [e[1] for e in distances[0:k]]
	return ret

def get_neighbor_d(instance, neighbor, ranges, attribs):
	"""
	Returns the distance between instance and neighbor.
	The distance between two instances is the sum of the Euclidean distances between the normalized values of each attribute in attribs. Non-Ordered attributes have a distance 1 if their values are equal, else 0.
	"""
	d = 0
	for attrib in attribs:
		if float(ranges[attrib]) == 0:
			None
		else:
			try:
				d += get_euclidean_d(float(instance[attrib])/float(ranges[attrib]), float(neighbor[attrib])/float(ranges[attrib]))
			except ValueError:
				if instance[attrib] == neighbor[attrib]:
					d += 1
	return d


def get_ranges(data_set, attribs = ["Cylinders", "Weight", "Acceleration"]):
	"""
	Returns a dictionary which has attribute labels as keys and the range of the attributes under that label in data_set as keys
	"""
	ranges = dict()
	for attrib in attribs:
		xs = []
		try:
			for inst in data_set:
					xs.append(float(inst[attrib]))
			ranges[attrib] = max(xs) - min(xs)
		except ValueError:
			ranges[attrib] = 1.0
	return ranges

def get_euclidean_d(i, i2):
	"""
	Returns the Euclidean distance between i and i2
	"""
	return float(math.sqrt(pow((i-i2),2)))
